{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e72ac01-2080-447b-bda4-34b49732dabb",
   "metadata": {},
   "source": [
    "# SynFlow Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa6275b4-69e3-4495-973f-cee5603cbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from Experiments import singleshot\n",
    "from Experiments import multishot\n",
    "from Experiments.theory import unit_conservation\n",
    "from Experiments.theory import layer_conservation\n",
    "from Experiments.theory import imp_conservation\n",
    "from Experiments.theory import schedule_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df964ef7-42ba-4b57-856e-1b04993806bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset = 'cifar10'\n",
    "    model = 'vgg16'\n",
    "    model_class = 'lottery'\n",
    "    dense_classifier = False\n",
    "    pretrained = False\n",
    "    optimizer = 'adam'\n",
    "    train_batch_size = 64\n",
    "    test_batch_size = 256\n",
    "    pre_epochs = 0\n",
    "    post_epochs = 1#10\n",
    "    train_epochs = 250\n",
    "    # what about stopping the IMP when the sparsity is reached?\n",
    "    lr = 0.001\n",
    "    lr_drops = tuple([])\n",
    "    lr_drop_rate = 0.1\n",
    "    weight_decay = 0.0\n",
    "    pruner= 'synflow'\n",
    "    compression= 1.0\n",
    "    prune_epochs= 1\n",
    "    compression_schedule= 'exponential'\n",
    "    mask_scope= 'global'\n",
    "    prune_dataset_ratio= 10\n",
    "    prune_batch_size= 256\n",
    "    prune_bias= False\n",
    "    prune_batchnorm= False\n",
    "    prune_residual= False\n",
    "    prune_train_mode= False\n",
    "    reinitialize= False\n",
    "    shuffle= False\n",
    "    invert= False\n",
    "    pruner_list= tuple([])\n",
    "    prune_epoch_list= tuple([])\n",
    "    compression_list= tuple([])\n",
    "    level_list= tuple([])\n",
    "    experiment = 'BK'\n",
    "    expid = True\n",
    "    result_dir = 'Results/data'\n",
    "    gpu = 1\n",
    "    workers =4\n",
    "    seed = 1\n",
    "    no_cuda= True#'store_true'\n",
    "    verbose= True#'store_true'\n",
    "    trial = 0\n",
    "    # Extra arguments for rewinding, Renda et al 2020(1)\n",
    "    rewind = 'LR' # Will there be rewinding, if so, what type? (None, 'LR', 'weight', 'NP')\n",
    "    ## Only pertinent for traditional rewinding as seen in Renda et al.\n",
    "    rewind_train = 20 # how many epochs of training before rewinding?  \n",
    "    rewind_epochs = 2 # how far back to rewind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b45b449-d008-4779-8509-70b6faa983aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "746bee0d-87d7-412f-9674-a9fa6f94aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In case of argument mistakes\n",
    "if args.rewind == None:\n",
    "    args.rewind = 'None'\n",
    "    args.rewind_train = None\n",
    "    args.rewind_epochs = None\n",
    "elif args.rewind == 'NP':\n",
    "    args.rewind_train = None\n",
    "    args.rewind_epochs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c38d29a3-863e-4580-9578-66e9b2b9d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expt ID: None_synflow_cifar10_vgg16_trial_0\n",
      "Pruner: synflow\n",
      "Rewind Method: None\n",
      "Train epochs before rewind: None\n",
      "Epochs to rewind: None\n"
     ]
    }
   ],
   "source": [
    "## Construct Result Directory ##\n",
    "if args.expid == False:\n",
    "    print(\"WARNING: this experiment is not being saved.\")\n",
    "    setattr(args, 'save', False)\n",
    "else:\n",
    "    expid = args.rewind+'_'+args.pruner+'_'+args.dataset+'_'+args.model+'_trial_'+str(args.trial)\n",
    "    result_dir = '{}/{}/{}'.format(args.result_dir, args.experiment, expid)\n",
    "    setattr(args, 'save', True)\n",
    "    setattr(args, 'result_dir', result_dir)\n",
    "#     try:\n",
    "    os.makedirs(result_dir)\n",
    "#     except FileExistsError:\n",
    "#         val = \"\"\n",
    "#         while val not in ['yes', 'no']:\n",
    "#             val = input(\"Experiment '{}' with expid '{}' exists.  Overwrite (yes/no)? \".format(args.experiment, args.expid))\n",
    "#         if val == 'no':\n",
    "#             quit()\n",
    "            \n",
    "print('Expt ID: ' + expid) \n",
    "print('Pruner: ' + args.pruner)\n",
    "print('Rewind Method: ' + args.rewind)\n",
    "print('Train epochs before rewind: ' + str(args.rewind_train))\n",
    "print('Epochs to rewind: ' + str(args.rewind_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9c6be-0013-47a0-b2b6-470ff856163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Args ##\n",
    "if args.save:\n",
    "    with open(args.result_dir + '/args.json', 'w') as f:\n",
    "        json.dump(args.__dict__, f, sort_keys=True, indent=4)\n",
    "\n",
    "## Run Experiment ##\n",
    "if args.experiment == 'BK':\n",
    "    \"\"\"\n",
    "    Custom experiment for the work done by Balwani & Krzyston 2021\n",
    "    Based off of singleshot.py & multishot.py seen in the Ganguli Lab SynFlow repo\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from Utils import load\n",
    "    from Utils import generator\n",
    "    from Utils import metrics\n",
    "    from train import *\n",
    "    from prune import *\n",
    "    from Experiments import singleshot\n",
    "    \n",
    "    \n",
    "    if args.rewind == None:\n",
    "        singleshot.run(args)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        ## Random Seed and Device ##\n",
    "        torch.manual_seed(args.seed)\n",
    "        device = load.device(args.gpu)\n",
    "\n",
    "        ## Data ##\n",
    "        print('Loading {} dataset.'.format(args.dataset))\n",
    "        input_shape, num_classes = load.dimension(args.dataset) \n",
    "        prune_loader = load.dataloader(args.dataset, args.prune_batch_size, True, args.workers, args.prune_dataset_ratio * num_classes)\n",
    "        train_loader = load.dataloader(args.dataset, args.train_batch_size, True, args.workers)\n",
    "        test_loader = load.dataloader(args.dataset, args.test_batch_size, False, args.workers)\n",
    "\n",
    "        ## Model ##\n",
    "        print('Creating {} model.'.format(args.model))\n",
    "        model = load.model(args.model, args.model_class)(input_shape, \n",
    "                                                         num_classes, \n",
    "                                                         args.dense_classifier,\n",
    "                                                         args.pretrained).to(device)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        opt_class, opt_kwargs = load.optimizer(args.optimizer)\n",
    "        optimizer = opt_class(generator.parameters(model), lr=args.lr, weight_decay=args.weight_decay, **opt_kwargs)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_drops, gamma=args.lr_drop_rate)\n",
    "\n",
    "        ## Save Original ##\n",
    "        torch.save(model.state_dict(),\"{}/model_init.pt\".format(args.result_dir))\n",
    "        torch.save(optimizer.state_dict(),\"{}/optimizer_init.pt\".format(args.result_dir))\n",
    "        torch.save(scheduler.state_dict(),\"{}/scheduler_init.pt\".format(args.result_dir))\n",
    "        \n",
    "        ## Counter for LR and weight rewinding\n",
    "        if args.rewind == 'LR' or args.rewind == 'weight':\n",
    "            count = 0\n",
    "        ## For NP rewinding, track all neural persistences \n",
    "        elif args.rewind == 'NP':\n",
    "            NPs = []\n",
    "        \n",
    "        # Every instance of pruning, prune a certain amount, stop when pruned at the specified level\n",
    "        for epoch in range(epochs):\n",
    "            count += 1\n",
    "            \n",
    "            # Train\n",
    "            model.train()\n",
    "            total = 0\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                train_loss = loss(output, target)\n",
    "                total += train_loss.item() * data.size(0)\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                if verbose & (batch_idx % log_interval == 0):\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                        100. * batch_idx / len(dataloader), train_loss.item()))\n",
    "            \n",
    "            # Eval\n",
    "            model.eval()\n",
    "            total = 0\n",
    "            correct1 = 0\n",
    "            correct5 = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in dataloader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    total += loss(output, target).item() * data.size(0)\n",
    "                    _, pred = output.topk(5, dim=1)\n",
    "                    correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
    "                    correct1 += correct[:,:1].sum().item()\n",
    "                    correct5 += correct[:,:5].sum().item()\n",
    "            average_loss = total / len(dataloader.dataset)\n",
    "            accuracy1 = 100. * correct1 / len(dataloader.dataset)\n",
    "            accuracy5 = 100. * correct5 / len(dataloader.dataset)\n",
    "            if verbose:\n",
    "                print('Evaluation: Average loss: {:.4f}, Top 1 Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "                    average_loss, correct1, len(dataloader.dataset), accuracy1))\n",
    "            \n",
    "            # Save weights\n",
    "            torch.save(model.state_dict(),\"{}/model_\"+epoch+\".pt\".format(args.result_dir))\n",
    "            \n",
    "            \n",
    "            # Rewind\n",
    "#             if args.rewind == 'NP':\n",
    "#                 # compute the neural persistence of the model\n",
    "#                 NP.append(metrics.neural_persistence(model))\n",
    "#                 # NP should always be increasing\n",
    "                \n",
    "                \n",
    "            elif count == args.rewind_train: #weight or LR rewind\n",
    "                if args.rewind == 'LR':\n",
    "                    # reset the learning rate\n",
    "                    scheduler.load_state_dict(torch.load(\"{}/scheduler_init.pt\".format(args.result_dir), map_location=device))\n",
    "                if args.rewind == 'weight':\n",
    "                    # determine which epoch to return to\n",
    "                    proper_epoch = args.rewind_train - args.rewind_epochs\n",
    "                    if proper_epoch == 0:\n",
    "                        proper_epoch = 'init'\n",
    "                    # reset weights to proper epoch\n",
    "                    model.load_state_dict(torch.load(\"{}/model_\"+proper_epoch+\".pt\".format(args.result_dir), map_location=device))\n",
    "                    # reset the learning rate to initial conditions\n",
    "                    scheduler.load_state_dict(torch.load(\"{}/scheduler_init.pt\".format(args.result_dir), map_location=device))\n",
    "                \n",
    "                # reset counter\n",
    "                count = 0\n",
    "            \n",
    "            # Prune Result\n",
    "            prune_result = metrics.summary(model, \n",
    "                                           pruner.scores,\n",
    "                                           metrics.flop(model, input_shape, device),\n",
    "                                           lambda p: generator.prunable(p, args.prune_batchnorm, args.prune_residual))\n",
    "            # Train Model\n",
    "            post_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                          test_loader, device, args.post_epochs, args.verbose)\n",
    "\n",
    "            # Save Data\n",
    "            post_result.to_pickle(\"{}/post-train-{}-{}-{}.pkl\".format(args.result_dir, args.pruner, str(compression),  str(level)))\n",
    "            prune_result.to_pickle(\"{}/compression-{}-{}-{}.pkl\".format(args.result_dir, args.pruner, str(compression), str(level)))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900b470-7fd0-4945-bfaa-914adc03a510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the weights of a model\n",
    "params = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        params[(name)] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733449e-22b5-46dd-8c8d-7434d67570da",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec18a93-d44e-476d-8f84-7d070c378976",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b7bf9-4f06-4603-aeb8-8fbab28660ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "- train all the way, THEN IMP\n",
    "- implement NP rewinding\n",
    "- compute total sparsity\n",
    "- test neural persistence code\n",
    "- \n",
    "- write paper\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4a6efe-c94a-42c6-bdaf-83d8ecf3950c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8adf888ba947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'module' is not defined"
     ]
    }
   ],
   "source": [
    "if isinstance(module, layers.Linear) or isinstance(module, nn.Linear):\n",
    "    print(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2deb670-c014-430a-b669-e2545ae025f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layers\n",
      "layers.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvModule' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-79f8c7bb2840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mind\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SynFlow/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    576\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvModule' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "for name, module in model.named_modules():\n",
    "    print(name)\n",
    "    if ind > 1:\n",
    "        print(module.in_features)\n",
    "    ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f7eb45d-6e90-4c7c-aaf6-47a69d5253e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.conv.kernel_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5793e626-c0fd-4cc6-8084-b7c00c6c7788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40c26ac8-58ea-4758-a32c-f37af613fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_all = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        weights_all.append(np.double(torch.max(torch.abs(param)).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5810aa5e-c754-42d3-bca0-0c6bedbada95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1864285469055176"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(weights_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2214a20-69a9-4f79-9f80-6cb5ae503030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8074cef8-df62-40a0-b50f-02ba249c74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9592870d-3027-4a2a-b6fe-f819cfbc0ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i%args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed0671f9-6ddc-4d5a-9472-3626478b68b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(args.rewind_train - args.rewind_epochs) +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056784e-6414-4055-b5c8-0e6c7e7f9cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
